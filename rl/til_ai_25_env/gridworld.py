import functools
from enum import IntEnum, StrEnum, auto

import gymnasium
import numpy as np
import pygame
from gymnasium.spaces import Box, Dict, Discrete
from gymnasium.utils.seeding import np_random
from mazelib import Maze
from mazelib.generate.DungeonRooms import DungeonRooms
from pettingzoo import AECEnv
from pettingzoo.utils import agent_selector, wrappers
from pettingzoo.utils.conversions import parallel_wrapper_fn
from pettingzoo.utils.env import ActionType, AgentID, ObsType
from supersuit import frame_stack_v2

from til_ai_25_env.wrappers.dict_flatten import FlattenDictWrapper

NUM_ITERS = 100


class Direction(IntEnum):
    RIGHT = 0
    DOWN = 1
    LEFT = 2
    UP = 3

    @property
    def movement(self) -> np.ndarray:
        match self:
            case Direction.RIGHT:
                return np.array([1, 0])
            case Direction.DOWN:
                return np.array([0, 1])
            case Direction.LEFT:
                return np.array([-1, 0])
            case Direction.UP:
                return np.array([0, -1])


class Action(IntEnum):
    FORWARD = 0
    BACKWARD = 1
    LEFT = 2
    RIGHT = 3
    STAY = 4


# every tile+player+wall combination is representable by a np.uint8
class Tile(IntEnum):
    NO_VISION = 0
    EMPTY = 1
    RECON = 2
    MISSION = 3


class Player(IntEnum):
    # do not exist as tiles in state, only in vision
    DEFENDER = 4  # 2**2
    ATTACKER = 8  # 2**3


class Wall(IntEnum):
    RIGHT = 4  # 2**4 = 16
    BOTTOM = 5  # 2**5 = 32
    LEFT = 6  # 2**6 = 64
    TOP = 7  # 2**7 = 128

    @property
    def power(self) -> int:
        return 2**self.value

    @property
    def orientation(self):
        # returns start x, start y, end x, end y
        # of the line for the wall to be drawn
        match self:
            case Wall.RIGHT:
                return (1, 0, 1, 1)
            case Wall.BOTTOM:
                return (0, 1, 1, 1)
            case Wall.LEFT:
                return (0, 0, 0, 1)
            case Wall.TOP:
                return (0, 0, 1, 0)

    @property
    def end_tile(self):
        # returns x, y of the tile on the other side of the wall
        match self:
            case Wall.RIGHT:
                return (1, 0)
            case Wall.BOTTOM:
                return (0, 1)
            case Wall.LEFT:
                return (-1, 0)
            case Wall.TOP:
                return (0, -1)


class RewardNames(StrEnum):
    ATTACKER_WINS = auto()
    ATTACKER_CAPTURES = auto()
    DEFENDER_CAPTURED = auto()
    DEFENDER_RECON = auto()
    DEFENDER_MISSION = auto()
    WALL_COLLISION = auto()
    AGENT_COLLIDER = auto()
    AGENT_COLLIDEE = auto()
    STATIONARY_PENALTY = auto()


DEFAULT_REWARDS_DICT = {
    RewardNames.ATTACKER_WINS: 10,
    RewardNames.ATTACKER_CAPTURES: 50,
    RewardNames.DEFENDER_CAPTURED: -50,
    RewardNames.DEFENDER_RECON: 1,
    RewardNames.DEFENDER_MISSION: 5,
}


# helpers
def manhattan(arr1, arr2):
    return np.sum(np.abs(arr1 - arr2))


def get_bit(value, bit_index):
    return (value >> bit_index) & 1


# for viewcone
def is_diagonal(a, b):
    return abs(a[0] - b[0]) == 1 and abs(a[1] - b[1]) == 1


def supercover_line(start: np.ndarray, end: np.ndarray) -> list[tuple[int, int]]:
    """Returns a list of (x, y) tiles the line passes through."""
    x0, y0 = start[0], start[1]
    x1, y1 = end[0], end[1]
    tiles = []

    # get x and y distance you need to pass through
    dx = x1 - x0
    dy = y1 - y0

    nx = abs(dx)
    ny = abs(dy)

    sign_x = 1 if dx > 0 else -1 if dx < 0 else 0
    sign_y = 1 if dy > 0 else -1 if dy < 0 else 0

    # starting tile
    px, py = x0.item(), y0.item()
    tiles.append((px, py))

    # distance travelled so far on each axis, ix and iy
    ix = iy = 0
    while ix < nx or iy < ny:
        # check for diagonal tile case
        # fixes asymmetric tile visibility around perfectly diagonal corners
        if (1 + 2 * ix) * ny == (1 + 2 * iy) * nx:
            px += sign_x
            py += sign_y
            ix += 1
            iy += 1
        elif (1 + 2 * ix) * ny < (1 + 2 * iy) * nx:
            px += sign_x
            ix += 1
        else:
            py += sign_y
            iy += 1
        tiles.append((px, py))

    return tiles


# maze generation
def convert_tile_to_edge(arena: np.ndarray, grid: np.ndarray):
    """
    converts a numpy grid maze setup as generated by mazelib to the edge-centric style the env expects,
    and modifies the input `arena` parameter inplace
    """
    _grid = grid.astype(dtype=np.uint8)
    # turn all the outside edges into the appropriate walls
    arena += _grid[1::2, 0:-2:2] * Wall.TOP.power
    arena += _grid[1::2, 2::2] * Wall.BOTTOM.power
    arena += _grid[0:-2:2, 1::2] * Wall.LEFT.power
    arena += _grid[2::2, 1::2] * Wall.RIGHT.power


def env(render_mode=None, stack_size: int = 4, stack_dim: int = -1, **kwargs):
    """
    The env function often wraps the environment in wrappers by default.
    You can find full documentation for these methods
    elsewhere in the developer documentation.
    """
    internal_render_mode = render_mode if render_mode != "ansi" else "human"
    env = raw_env(render_mode=internal_render_mode, **kwargs)
    # This wrapper is only for environments which print results to the terminal
    if render_mode == "ansi":
        env = wrappers.CaptureStdoutWrapper(env)
    # wrapper flattens the dict
    env = FlattenDictWrapper(env)
    # configurable frame stacking
    env = frame_stack_v2(env, stack_size, stack_dim)
    # this wrapper helps error handling for discrete action spaces
    env = wrappers.AssertOutOfBoundsWrapper(env)
    # Provides a wide variety of helpful user errors
    # Strongly recommended
    env = wrappers.OrderEnforcingWrapper(env)
    return env


parallel_env = parallel_wrapper_fn(env)


class raw_env(AECEnv[AgentID, ObsType, ActionType]):
    """
    The metadata holds environment constants. From gymnasium, we inherit the "render_modes",
    metadata which specifies which modes can be put into the render() method.
    At least human mode should be supported.
    The "name" metadata allows the environment to be pretty printed.
    """

    metadata = {
        "render_modes": ["human"],
        "name": "til_env",
        "render_fps": 10,
        "is_parallelizable": True,
    }

    def __init__(
        self,
        render_mode=None,
        possible_agents: list[AgentID] = ["player_" + str(r) for r in range(4)],
        size: int = 16,
        mission_prob: float = 0.2,
        wall_prob: float = 0.2,
        window_size: int = 768,
        viewcone: tuple[int, int, int, int] = (1, 1, 1, 3),
        rewards_dict: dict[str, int] = DEFAULT_REWARDS_DICT,
    ):
        """
        The init method takes in environment arguments and defines the following attributes:
        - render_mode: render mode to be used
        - possible_agents: names of all the agents
        - size: number of gridsquares for one axis (e.g. 5 would be a 5x5 grid, 10 would be a 10x10, etc.)
        - viewcone: parameters for the view distance from left, right, back, and front respectively
        """
        # initialize grid
        self.size = size  # size of the square grid
        self.window_size = window_size  # size of the PyGame window
        self.possible_agents = possible_agents
        self.viewcone = viewcone

        # initialize random generation parameters
        self.mission_prob = mission_prob
        self.wall_prob = wall_prob

        # viewcone
        self.viewcone_width = self.viewcone[0] + self.viewcone[1] + 1
        self.viewcone_length = self.viewcone[2] + self.viewcone[3] + 1

        # mapping between agent name and ID
        self.agent_name_mapping = dict(
            zip(self.possible_agents, list(range(len(self.possible_agents))))
        )

        assert render_mode is None or render_mode in self.metadata["render_modes"]
        self.render_mode = render_mode
        """
        If human-rendering is used, `self.window` will be a reference
        to the window that we draw to. `self.clock` will be a clock that is used
        to ensure that the environment is rendered at the correct framerate in
        human-mode. They will remain `None` until human-mode is used for the
        first time.
        """
        self.window = None
        self.clock = None
        self.font = None

        # initialize selector for defender
        self._defender_selector = agent_selector(self.possible_agents)

        # set rewards dictionary
        self.rewards_dict = rewards_dict

        # initialize random
        self._np_random, self._np_random_seed = None, None

        # initialize walls
        # walls in this case are ordered pairs of coordinates
        self.walls: set[tuple[tuple[int, int], tuple[int, int]]] = set()
        self._maze = Maze()

    # memoize/cache observation and action spaces since they don't change
    # gymnasium spaces are defined and documented here: https://gymnasium.farama.org/api/spaces/
    # TODO: hearing sounds, attackers knowing where each other are
    @functools.lru_cache(maxsize=None)
    def observation_space(self, agent: AgentID):
        return Dict(
            {
                "viewcone": Box(
                    0,
                    2**8 - 1,
                    shape=(
                        self.viewcone_length,
                        self.viewcone_width,
                    ),
                    dtype=np.uint8,
                ),
                "direction": Discrete(4),
                "defender": Discrete(2),
            }
        )

    @functools.lru_cache(maxsize=None)
    def action_space(self, agent: AgentID):
        return Discrete(len(Action))

    def render(self):
        """
        Renders the environment. In human mode, it can print to terminal, open
        up a graphical window, or open up some other display that a human can see and understand.
        """
        if self.render_mode is None:
            gymnasium.logger.warn(
                "You are calling render method without specifying any render mode."
            )
            return

        if self.render_mode in ("rgb_array", "human"):
            return self._render_frame()

    def _render_frame(self):
        if self.window is None and self.render_mode == "human":
            pygame.init()
            pygame.display.init()
            self.window = pygame.display.set_mode((self.window_size, self.window_size))
        if self.clock is None and self.render_mode == "human":
            self.clock = pygame.time.Clock()
        if self.font is None:
            self.font = pygame.font.Font("freesansbold.ttf", 12)

        canvas = pygame.Surface((self.window_size, self.window_size))
        canvas.fill((255, 255, 255))
        pix_square_size = (
            self.window_size / self.size
        )  # The size of a single grid square in pixels

        # add gridlines
        for x in range(self.size + 1):
            pygame.draw.line(
                canvas,
                (211, 211, 211),
                (0, pix_square_size * x),
                (self.window_size, pix_square_size * x),
                width=3,
            )
            pygame.draw.line(
                canvas,
                (211, 211, 211),
                (pix_square_size * x, 0),
                (pix_square_size * x, self.window_size),
                width=3,
            )

        # draw environment tiles
        for x, y in np.ndindex((self.size, self.size)):
            tile = self._state[x, y]
            # draw whether the tile contains points
            match tile % 4:
                case Tile.RECON.value:
                    pygame.draw.circle(
                        canvas,
                        (255, 165, 0),
                        ((x + 0.5) * pix_square_size, (y + 0.5) * pix_square_size),
                        pix_square_size / 10,
                    )
                case Tile.MISSION.value:
                    pygame.draw.circle(
                        canvas,
                        (147, 112, 219),
                        ((x + 0.5) * pix_square_size, (y + 0.5) * pix_square_size),
                        pix_square_size / 6,
                    )
            # draw walls
            for wall in Wall:
                if not get_bit(tile, wall.value):
                    continue
                pygame.draw.line(
                    canvas,
                    0,
                    (
                        pix_square_size * (x + wall.orientation[0]),
                        pix_square_size * (y + wall.orientation[1]),
                    ),
                    (
                        pix_square_size * (x + wall.orientation[2]),
                        pix_square_size * (y + wall.orientation[3]),
                    ),
                    width=7,
                )

        # draw all the players
        for agent, location in self.agent_locations.items():
            center = (location + 0.5) * pix_square_size
            pygame.draw.circle(
                canvas,
                (0, 0, 255) if agent == self.defender else (255, 0, 0),
                center,
                pix_square_size / 3,
            )
            # draw direction indicator
            pygame.draw.line(
                canvas,
                (0, 255, 0),
                center,
                (
                    location
                    + 0.5
                    + Direction(self.agent_directions[agent]).movement * 0.33
                )
                * pix_square_size,
                3,
            )
            text = self.font.render(agent[-1], True, "black")
            rect = text.get_rect(center=center)
            canvas.blit(text, rect)

        if self.render_mode == "human":
            # The following line copies our drawings from `canvas` to the visible window
            self.window.blit(canvas, canvas.get_rect())
            pygame.event.pump()
            pygame.display.update()

            # We need to ensure that human-rendering occurs at the predefined framerate.
            # The following line will automatically add a delay to
            # keep the framerate stable.
            self.clock.tick(self.metadata["render_fps"])

    def _view_to_world(self, agent: AgentID, view_coord: np.ndarray) -> np.ndarray:
        """
        map viewcone coordinate to world coordinate
        """
        agent_loc = self.agent_locations[agent]
        agent_dir = self.agent_directions[agent]
        match Direction(agent_dir):
            case Direction.RIGHT:
                return agent_loc + view_coord
            case Direction.DOWN:
                return agent_loc - np.array((view_coord[1], -view_coord[0]))
            case Direction.LEFT:
                return agent_loc - view_coord
            case Direction.UP:
                return agent_loc + np.array((view_coord[1], -view_coord[0]))

    def _world_to_view(self, agent, world_coord: np.ndarray) -> np.ndarray:
        """
        map world coordinate to viewcone coordinate
        """
        agent_loc = self.agent_locations[agent]
        agent_dir = self.agent_directions[agent]
        match Direction(agent_dir):
            case Direction.RIGHT:
                return world_coord - agent_loc
            case Direction.DOWN:
                return np.array(
                    ((world_coord[1] - agent_loc[1]), (agent_loc[0] - world_coord[0]))
                )
            case Direction.LEFT:
                return agent_loc - world_coord
            case Direction.UP:
                return np.array(
                    ((agent_loc[1] - world_coord[1]), (world_coord[0] - agent_loc[0]))
                )

    def _idx_to_view(self, idx: tuple[int, int]) -> np.ndarray:
        return np.array(idx) - np.array((self.viewcone[0], self.viewcone[2]))

    def _view_to_idx(self, view_coord: np.ndarray) -> tuple[int, int]:
        # using .item() to return a native python int, mostly just for neater debug printing
        return (
            view_coord[0].item() + self.viewcone[0],
            view_coord[1].item() + self.viewcone[2],
        )

    def _is_idx_valid(self, idx: np.ndarray):
        return (0 <= idx[0] < self.viewcone_length) and (
            0 <= idx[1] < self.viewcone_width
        )

    def _is_world_coord_valid(self, world_coord: np.ndarray):
        return (0 <= world_coord[0] < self.size) and (0 <= world_coord[1] < self.size)

    def _is_visible(self, agent, end: np.ndarray):
        start = self.agent_locations[agent]
        # if it's a self-to-self check, return true
        if (start == end).all():
            return True

        # get tiles covered by LOS line from start to end
        path = supercover_line(start, end)
        for i in range(len(path) - 1):
            tile, next_tile = path[i], path[i + 1]

            # check if tile is diagonal
            if tile[0] - next_tile[0] != 0 and tile[1] - next_tile[1] != 0:
                horiz0 = tuple(sorted((tile, (next_tile[0], tile[1]))))
                horiz1 = tuple(sorted(((next_tile[0], tile[1]), next_tile)))
                vert0 = tuple(sorted((tile, (tile[0], next_tile[1]))))
                vert1 = tuple(sorted(((tile[0], next_tile[1]), next_tile)))

                # terminate if neither horiz nor vert direction is open
                # allows for (and essentially hardcodes) corner-peeking
                if (horiz0 in self.walls or horiz1 in self.walls) and (
                    vert0 in self.walls or vert1 in self.walls
                ):
                    return False
            else:
                # if not diagonal, check the edge normally
                edge = tuple(sorted((tile, next_tile)))
                if edge in self.walls:
                    return False
        # if you made it to the end, you have visibility
        return True

    def observe(self, agent):
        """
        Observe should return the observation of the specified agent. This function
        should return a sane observation (though not necessarily the most up to date possible)
        at any time after reset() is called.
        """
        view = np.zeros((self.viewcone_length, self.viewcone_width), dtype=np.uint8)
        for idx in np.ndindex((self.viewcone_length, self.viewcone_width)):
            view_coord = self._idx_to_view(np.array(idx))
            world_coord = self._view_to_world(agent, view_coord)
            if not self._is_world_coord_valid(world_coord):
                continue
            # check if tile is visible
            if self._is_visible(agent, world_coord):
                # TODO: filter state to only include the walls you can see
                view[idx] = self._state[*world_coord]

        # add players
        for _agent, loc in self.agent_locations.items():
            view_coord = self._world_to_view(agent, loc)
            idx = self._view_to_idx(view_coord)
            # print(f"from {agent} pov, {_agent} at {loc} at view {view_coord} idx {idx}")
            if self._is_idx_valid(idx) and view[idx] != 0:
                view[idx] += (
                    np.uint8(Player.DEFENDER)
                    if _agent == self.defender
                    else np.uint8(Player.ATTACKER)
                )
        # print(
        #     f"view for agent {agent} facing {Direction(self.agent_directions[agent]).name}"
        # )
        # print(view)
        return {
            "viewcone": view,
            "direction": self.agent_directions[agent],
            "defender": 1 if agent == self.defender else 0,
        }

    def close(self):
        """
        Close should release any graphical displays, subprocesses, network connections
        or any other environment data which should not be kept around after the
        user is no longer using the environment.
        """
        if self.window is not None:
            pygame.display.quit()
            pygame.quit()

    def _add_wall(self, a: tuple[int, int], b: tuple[int, int]):
        self.walls.add(tuple(sorted([a, b])))

    def _add_walls(self):
        # reset existing walls
        self.walls = set()

        # iterate over arena state and add the walls on each tile
        for x, y in np.ndindex((self.size, self.size)):
            tile = self._arena[x, y]
            for wall in Wall:
                if not get_bit(tile, wall.value):
                    continue
                end_tile = wall.end_tile
                self._add_wall((x, y), (x + end_tile[0], y + end_tile[1]))

    def _random_room(self, min_dim: int = 2, max_dim: int = 5):
        # generate starting point
        start = self._np_random.integers(0, self.size - max_dim, size=2)
        end = np.array(
            (
                self._np_random.integers(start[0] + min_dim, start[0] + max_dim),
                self._np_random.integers(start[1] + min_dim, start[1] + max_dim),
            )
        )
        # randomly flip the axes
        if self._np_random.random() >= 0.5:
            return [tuple(start * 2 + 1), tuple(end * 2 + 1)]
        else:
            return [tuple(np.flip(start) * 2 + 1), tuple(np.flip(end) * 2 + 1)]

    def _new_maze_generator(self, min_rooms: int = 5, max_rooms: int = 10):
        # generate rooms for maze
        # starting 3x3 home base room for defender in the top-left corner
        rooms = [[(1, 1), (5, 5)]]
        for _ in range(self._np_random.integers(min_rooms, max_rooms)):
            rooms.append(self._random_room())
        return DungeonRooms(self.size, self.size, rooms=rooms)

    # randomly generate new arena
    def _generate_arena(self, options=None):
        self._maze.generator = self._new_maze_generator()
        self._maze.generate()
        # randomly knock down some walls to open up new pathways
        # get all indices of the walls, excluding exterior walls
        _grid = self._maze.grid.copy()
        _grid[0, :] = 0  # top edge
        _grid[-1, :] = 0  # bottom edge
        _grid[:, 0] = 0  # left edge
        _grid[:, -1] = 0  # right edge
        # exclude corner bits
        _grid = _grid * (np.indices(_grid.shape).sum(axis=0) % 2)

        walls = np.where(_grid == 1)
        # drop some % of walls to open up more pathways
        idx = self._np_random.choice(
            walls[0].shape[0],
            size=int(walls[0].shape[0] * self.wall_prob),
            replace=False,
        )
        self._maze.grid[walls[0][idx], walls[1][idx]] = 0

        # add recon and mission tiles
        self._arena = self._np_random.choice(
            (np.uint8(Tile.RECON), np.uint8(Tile.MISSION)),
            size=(self.size, self.size),
            p=(1 - self.mission_prob, self.mission_prob),
        )
        convert_tile_to_edge(self._arena, self._maze.grid)
        self._add_walls()

        # select starting directions and locations
        self.starting_directions = self._np_random.integers(0, 4, size=4)
        self.starting_locations = np.array(
            [
                (0, 0),
                (
                    self._np_random.integers(0, self.size // 2),
                    self._np_random.integers(self.size // 2, self.size),
                ),
                (
                    self._np_random.integers(self.size // 2, self.size),
                    self._np_random.integers(0, self.size // 2),
                ),
                self._np_random.integers(self.size // 2, self.size, size=2),
            ],
        )

    # reset state to pregenerated arena
    def _reset_state(self, options=None):
        self._state = self._arena.copy()
        _dirs: dict[AgentID, np.int64] = {self.defender: self.starting_directions[0]}
        _locs: dict[AgentID, np.ndarray] = {self.defender: self.starting_locations[0]}
        for i, agent in enumerate([a for a in self.agents if a != self.defender], 1):
            _dirs[agent] = self.starting_directions[i]
            _locs[agent] = self.starting_locations[i]
        self.agent_directions = _dirs
        self.agent_locations = _locs

    def reset(self, seed=None, options=None):
        """
        Reset needs to initialize the following attributes
        - agents
        - rewards
        - _cumulative_rewards
        - terminations
        - truncations
        - infos
        - agent_selection
        And must set up the environment so that render(), step(), and observe()
        can be called without issues.
        Here it sets up the state dictionary which is used by step()
        and the observations dictionary which is used by step() and observe()
        """
        if self._np_random is None or seed is not None:
            self._np_random, self._np_random_seed = np_random(seed)
            self._maze.set_seed(seed)
            print(f"seeded with {self._np_random_seed}")

        # agent_selector utility cyclically steps through agents list
        self.agents = self.possible_agents[:]
        self._agent_selector = agent_selector(self.agents)
        self.agent_selection = self._agent_selector.next()
        # select the next player to be the defender
        self.defender: AgentID = self._defender_selector.next()
        if self._defender_selector.is_first() or self._arena is None:
            self._generate_arena(options)

        self._reset_state(options)
        self.rewards = {agent: 0 for agent in self.agents}
        self._cumulative_rewards = {agent: 0 for agent in self.agents}
        self.terminations = {agent: False for agent in self.agents}
        self.truncations = {agent: False for agent in self.agents}
        self.infos = {agent: {} for agent in self.agents}
        self._actions: dict[AgentID, Action] = {}
        self.observations = {agent: self.observe(agent) for agent in self.agents}
        self.num_moves = 0

    def state(self):
        """
        Returns the underlying state of the environment, provided for debugging.
        NOT to be used in training, as it contains information that agents don't know
        """
        return self._state

    # reward actions
    # TODO make attacker rewards based on distance?
    def _capture_defender(self, capturers):
        """
        Given a list of agents who captured the defender, processes those agents' capture of the defender.
        Terminates the game and gives attackers and the defender corresponding rewards.
        """
        # defender gets captured, terminate and reward attacker
        self.terminations = {agent: True for agent in self.agents}
        for agent in self.agents:
            if agent == self.defender:
                self.rewards[self.defender] += self.rewards_dict.get(
                    RewardNames.DEFENDER_CAPTURED, 0
                )
                continue
            self.rewards[agent] += self.rewards_dict.get(RewardNames.ATTACKER_WINS, 0)
            if agent in capturers:
                self.rewards[agent] += self.rewards_dict.get(
                    RewardNames.ATTACKER_CAPTURES, 0
                )

    def _handle_agent_collision(self, agent1: AgentID, agent2: AgentID):
        """
        Given two agents, handle agent1 colliding into agent2
        """
        self.rewards[agent1] += self.rewards_dict.get(RewardNames.AGENT_COLLIDER, 0)
        self.rewards[agent2] += self.rewards_dict.get(RewardNames.AGENT_COLLIDEE, 0)

    def _handle_wall_collision(self, agent: AgentID):
        self.rewards[agent] += self.rewards_dict.get(RewardNames.WALL_COLLISION, 0)

    # game rules
    def _enforce_collisions(
        self, agent: AgentID, direction: Direction
    ) -> tuple[np.ndarray, bool]:
        """
        given agent and direction of movement, return:
        * resultant movement direction, and
        * the AgentID of any agent collided into, or None if no collision happened
        """
        # check walls on agent's tile
        tile = self._state[*self.agent_locations[agent]]
        # add 4 bc direction+4=wall bit
        if get_bit(tile, direction.value + 4):
            self._handle_wall_collision(agent)
            return np.array([0, 0]), None

        # if good, check reverse wall on destination tile
        next_loc = self.agent_locations[agent] + direction.movement
        next_tile = self._state[*next_loc]
        if get_bit(next_tile, ((direction.value + 2) % 4) + 4):
            self._handle_wall_collision(agent)
            return np.array([0, 0]), None

        # if that's good too, check for agent collisions
        for _agent, _loc in self.agent_locations.items():
            if _agent == agent:
                continue
            if (next_loc == _loc).all():
                self._handle_agent_collision(agent, _agent)
                # return the collidee
                return np.array([0, 0]), _agent

        # checks out, return original direction
        return direction.movement, None

    def _move_agent(self, agent: AgentID, action: int):
        """
        Updates agent location, accruing rewards along the way
        return the name of the agent collided into, or None
        """
        _action = Action(action)
        if _action in (Action.FORWARD, Action.BACKWARD):
            _direction = Direction(
                self.agent_directions[agent]
                if _action is Action.FORWARD
                else (self.agent_directions[agent] + 2) % 4
            )
            # enforce collisions with walls and other agents
            direction, collision = self._enforce_collisions(agent, _direction)
            # use np.clip to not leave grid
            self.agent_locations[agent] = np.clip(
                self.agent_locations[agent] + direction, 0, self.size - 1
            )
            # update defender rewards
            if agent == self.defender:
                x, y = self.agent_locations[agent]
                tile = self._state[x, y]
                match Tile(tile % 4):
                    case Tile.RECON:
                        self.rewards[self.defender] += self.rewards_dict.get(
                            RewardNames.DEFENDER_RECON, 0
                        )
                        self._state[x, y] -= Tile.RECON.value - Tile.EMPTY.value
                    case Tile.MISSION:
                        self.rewards[self.defender] += self.rewards_dict.get(
                            RewardNames.DEFENDER_MISSION, 0
                        )
                        self._state[x, y] -= Tile.MISSION.value - Tile.EMPTY.value
            return collision
        if _action in (Action.LEFT, Action.RIGHT):
            # update direction of agent, right = +1 and left = -1 (which is equivalent to +3), mod 4.
            self.agent_directions[agent] = (
                self.agent_directions[agent] + (3 if _action is Action.LEFT else 1)
            ) % 4
        if _action is (Action.STAY):
            # apply stationary penalty
            self.rewards[agent] += self.rewards_dict.get(
                RewardNames.STATIONARY_PENALTY, 0
            )
        return None

    def _handle_actions(self):
        """
        Handles all actions at once, handling attacker-defender captures
        """
        capturers = []
        defender_action = self._actions.pop(self.defender)

        # update defender location first
        def_collision = self._move_agent(self.defender, defender_action)
        # if the defender walks into an attacker, mark that attacker as a capturer
        if def_collision is not None:
            capturers.append(def_collision)

        # now update all the attackers
        for agent, action in self._actions.items():
            # update agent location
            agent_collision = self._move_agent(agent, action)
            # if this attacker walks into the defender, mark it as a capturer
            if agent_collision == self.defender:
                capturers.append(agent)

        # if there are any capturers, capture the defender
        if len(capturers) > 0:
            self._capture_defender(capturers)

        # clear actions
        self._actions = {}

    def get_info(self, agent: AgentID):
        """
        Returns accessory info for training/reward shaping
        """
        return {
            "distance": np.linalg.norm(
                self.agent_locations[agent] - self.agent_locations[self.defender],
                ord=1,
            ),
            "manhattan": manhattan(
                self.agent_locations[agent],
                self.agent_locations[self.defender],
            ),
        }

    def step(self, action: ActionType):
        """
        step(action) takes in an action for the current agent (specified by
        agent_selection) and needs to update
        - rewards
        - _cumulative_rewards (accumulating the rewards)
        - terminations
        - truncations
        - infos
        - agent_selection (to the next agent)
        And any internal state used by observe() or render()
        """
        if (
            self.terminations[self.agent_selection]
            or self.truncations[self.agent_selection]
        ):
            # handles stepping an agent which is already dead
            # accepts a None action for the one agent, and moves the agent_selection to
            # the next dead agent, or if there are no more dead agents, to the next live agent
            self._was_dead_step(action)
            return

        agent = self.agent_selection

        # the agent which stepped last had its _cumulative_rewards accounted for
        # (because it was returned by last()), so the _cumulative_rewards for this
        # agent should start again at 0
        self._cumulative_rewards[agent] = 0

        # stores action of current agent
        self._actions[self.agent_selection] = action

        # handle actions and rewards if it is the last agent to act
        if self._agent_selector.is_last():
            # execute actions
            self._handle_actions()

            self.num_moves += 1
            # The truncations dictionary must be updated for all players.
            self.truncations = {
                agent: self.num_moves >= NUM_ITERS for agent in self.agents
            }

            # observe the current state and get new infos
            for agent in self.agents:
                self.observations[agent] = self.observe(agent)
                # update infos
                self.infos[agent] = self.get_info(agent)
        else:
            # no rewards are allocated until all players give an action
            self._clear_rewards()

        # selects the next agent.
        self.agent_selection = self._agent_selector.next()
        # Adds .rewards to ._cumulative_rewards
        self._accumulate_rewards()

        if self.render_mode == "human":
            self.render()
